# Publications
<ol>

  <li><b>StreamingThinker: Large Language Models Can Think While Reading.</b>
  <a href="https://arxiv.org/abs/2510.17238" target="_blank" rel="noopener noreferrer">[PDF]</a>
  <a href="https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker" target="_blank" rel="noopener noreferrer">[Code]</a><br> 
	  <i><u>Junlong Tong</u>, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen.</i><br>  
      arXiv 2025.<br> 
    </li>

  <li><b>VisiPruner: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs.</b>
  <a href="https://arxiv.org/abs/2510.17205" target="_blank" rel="noopener noreferrer">[PDF]</a><br> 
  <!-- <a href="https://github.com/EIT-NLP/StreamingLLM" target="_blank" rel="noopener noreferrer">[Code]</a><br>  -->
	  <i>Yingqi Fan, Anhao Zhao, Jinlan Fu, <u>Junlong Tong</u>, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen.</i><br> 
      EMNLP 2025.<br> 
    </li>

  <li><b>SkipGPT: Each Token is One of a Kind.</b>
  <a href="https://arxiv.org/abs/2506.04179" target="_blank" rel="noopener noreferrer">[PDF]</a>
  <a href="https://github.com/EIT-NLP/SkipGPT" target="_blank" rel="noopener noreferrer">[Code]</a><br> 
	  <i>Anhao Zhao, Fanghua Ye, Yingqi Fan, <u>Junlong Tong</u>, Jing Xiong, Zhiwei Fei, Hui Su, Xiaoyu Shen.</i><br> 
      ICML 2025.<br> 
    </li>


  <li><b>LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding.</b>
  <a href="https://arxiv.org/abs/2505.16983" target="_blank" rel="noopener noreferrer">[PDF]</a>
  <a href="https://github.com/EIT-NLP/StreamingLLM" target="_blank" rel="noopener noreferrer">[Code]</a><br> 
	  <i><u>Junlong Tong</u>, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen.</i><br> 
      Findings of ACL 2025.<br> 
    </li>

  <li><b>Context Guided Transformer Entropy Modeling for Video Compression.</b>
  <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tong_Context_Guided_Transformer_Entropy_Modeling_for_Video_Compression_ICCV_2025_paper.html" target="_blank" rel="noopener noreferrer">[PDF]</a><br> 
  <!-- <a href="https://github.com/EIT-NLP/StreamingLLM" target="_blank" rel="noopener noreferrer">[Code]</a><br>  -->
	  <i><u>Junlong Tong</u>, Wei Zhang, Yaohui Jin, Xiaoyu Shen.</i><br> 
      ICCV 2025.<br> 
    </li>

<li><b>Probabilistic Decomposition Transformer for Time Series Forecasting.</b>
  <a href="https://doi.org/10.1137/1.9781611977653.ch54" target="_blank" rel="noopener noreferrer">[PDF]</a>
  <a href="https://github.com/JL-tong/PDTrans" target="_blank" rel="noopener noreferrer">[Code]</a><br> 
	  <i><u>Junlong Tong</u>, Liping Xie, Kanjian Zhang.</i><br> 
      SIAM International Conference on Data Mining (SDM 2023).<br> 
	<!-- Submit to SIAM International Conference on Data Mining (SDM2023)<br>  -->
    </li>
    
  <li><b>Enhancing Time Series Forecasting: A Hierarchical Transformer with Probabilistic Decomposition Representation.</b>
  <a href="https://doi.org/10.1016/j.ins.2023.119410" target="_blank" rel="noopener noreferrer">[PDF]</a><br>
  <i><u>Junlong Tong</u>, Liping Xie, Wankou Yang, Kanjian Zhang, Junsheng Zhao.</i><br> 
  Information Sciences 2023.<br> 
    </li>
  
  <li><b>Hourly Solar Irradiance Forecasting Based on Encoderâ€“decoder Model Using Series Decomposition and Dynamic Error Compensation.</b>
	<a href="https://jl-tong.github.io/docs/ECM-hourly.pdf" target="_blank" rel="noopener noreferrer">[PDF]</a><br> 
	<i><u>Junlong Tong</u>, Liping Xie, Shixiong Fang, Wankou Yang, Kanjian Zhang.</i><br> 
	Energy Conversion and Management 2022.<br> 
    </li>
</ol>
